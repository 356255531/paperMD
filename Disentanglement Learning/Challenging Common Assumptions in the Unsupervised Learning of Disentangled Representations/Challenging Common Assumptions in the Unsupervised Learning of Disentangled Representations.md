# Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations
Authors: Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem
Venue: Proceedings of the 36th International Conference on Machine Learning_, PMLR 97:4114-4124, 2019.
## Comments

## Summary
The author theoretically shows that the unsupervised learning of disentangled representations is fundamentally ***impossible*** without inductive biases on both the models and the data.

The experiments show that the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly ***cannot*** be identified without supervision.

The author suggests that disentanglement learning works should be ***explicit*** about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations

## Background
## Problem
Disentangle factors from data using unsupervised learning, when data is generated by explanatory factors (substantially lower dimensional and semantically meaningful latent variable).
## Approach
## Experiment
## Conclusion 
